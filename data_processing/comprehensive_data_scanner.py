#!/usr/bin/env python3
"""
Comprehensive Data Scanner
Divine Calculus Engine - Complete Data Analysis & Aggregation

This system scans and analyzes all data generated across the entire Divine Calculus Engine,
including training results, pattern analysis, scientific breakthroughs, and system outputs.
"""

import os
import json
import time
import hashlib
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime
from collections import defaultdict
import glob
import re

@dataclass
class DataSource:
    """Data source information"""
    name: str
    file_path: str
    file_type: str
    size_bytes: int
    last_modified: float
    content_summary: Dict[str, Any]
    data_quality_score: float

@dataclass
class SystemData:
    """Complete system data analysis"""
    total_files: int
    total_size_mb: float
    data_sources: List[DataSource]
    system_health_score: float
    data_coherence_score: float
    breakthrough_insights: List[str]
    cross_system_patterns: List[Dict[str, Any]]
    quantum_signatures: Dict[str, float]

class ComprehensiveDataScanner:
    """Comprehensive data scanner for the entire Divine Calculus Engine"""
    
    def __init__(self):
        self.scan_results = {}
        self.data_sources = []
        self.system_insights = []
        
        # File patterns to scan
        self.file_patterns = {
            'training_results': [
                'optimized_training_results_*.json',
                'breakthrough_optimization_results_*.json',
                'fast_optimized_training_results_*.json'
            ],
            'pattern_analysis': [
                'simplified_pattern_analysis_results_*.json',
                'multi_spectral_analysis_results_*.json'
            ],
            'scientific_data': [
                'enhanced_scientific_integration_*.json',
                'updated_agent_training_data_*.json'
            ],
            'validation_results': [
                'technical_validation_results_*.json',
                'aios_testing_results_*.json'
            ],
            'quantum_data': [
                'quantum_seed_generation_*.json',
                'comprehensive_ml_training_*.json'
            ],
            'system_outputs': [
                '*_SUMMARY.md',
                '*_ANALYSIS.md',
                '*_REPORT.md'
            ],
            'code_files': [
                '*.py',
                '*.js',
                '*.md',
                '*.json'
            ]
        }
        
    def scan_all_data(self) -> SystemData:
        """Scan all data across the entire system"""
        print("üîç COMPREHENSIVE DATA SCANNER")
        print("Divine Calculus Engine - Complete Data Analysis")
        print("=" * 70)
        
        # Step 1: Scan all file types
        print("\nüìÅ STEP 1: SCANNING ALL DATA FILES")
        all_files = self.scan_all_files()
        
        # Step 2: Analyze data sources
        print("\nüìä STEP 2: ANALYZING DATA SOURCES")
        data_sources = self.analyze_data_sources(all_files)
        
        # Step 3: Calculate system metrics
        print("\nüìà STEP 3: CALCULATING SYSTEM METRICS")
        system_metrics = self.calculate_system_metrics(data_sources)
        
        # Step 4: Generate insights
        print("\nüí° STEP 4: GENERATING CROSS-SYSTEM INSIGHTS")
        insights = self.generate_cross_system_insights(data_sources)
        
        # Step 5: Create comprehensive report
        print("\nüìã STEP 5: CREATING COMPREHENSIVE REPORT")
        system_data = self.create_system_data(data_sources, system_metrics, insights)
        
        return system_data
    
    def scan_all_files(self) -> Dict[str, List[str]]:
        """Scan all files in the system"""
        all_files = {}
        
        for category, patterns in self.file_patterns.items():
            category_files = []
            for pattern in patterns:
                files = glob.glob(pattern)
                category_files.extend(files)
            
            all_files[category] = category_files
            print(f"  üìÅ {category}: {len(category_files)} files")
        
        return all_files
    
    def analyze_data_sources(self, all_files: Dict[str, List[str]]) -> List[DataSource]:
        """Analyze all data sources"""
        data_sources = []
        
        for category, files in all_files.items():
            for file_path in files:
                try:
                    data_source = self.analyze_single_file(file_path, category)
                    if data_source:
                        data_sources.append(data_source)
                except Exception as e:
                    print(f"Error analyzing {file_path}: {e}")
        
        return data_sources
    
    def analyze_single_file(self, file_path: str, category: str) -> Optional[DataSource]:
        """Analyze a single data file"""
        try:
            # Get file stats
            stat = os.stat(file_path)
            file_size = stat.st_size
            last_modified = stat.st_mtime
            
            # Determine file type
            file_type = self.determine_file_type(file_path)
            
            # Analyze content
            content_summary = self.analyze_file_content(file_path, file_type)
            
            # Calculate data quality score
            data_quality_score = self.calculate_data_quality(file_path, content_summary)
            
            return DataSource(
                name=os.path.basename(file_path),
                file_path=file_path,
                file_type=file_type,
                size_bytes=file_size,
                last_modified=last_modified,
                content_summary=content_summary,
                data_quality_score=data_quality_score
            )
            
        except Exception as e:
            print(f"Error analyzing {file_path}: {e}")
            return None
    
    def determine_file_type(self, file_path: str) -> str:
        """Determine the type of file"""
        ext = os.path.splitext(file_path)[1].lower()
        
        if ext == '.json':
            return 'json_data'
        elif ext == '.py':
            return 'python_code'
        elif ext == '.js':
            return 'javascript_code'
        elif ext == '.md':
            return 'markdown_documentation'
        else:
            return 'other'
    
    def analyze_file_content(self, file_path: str, file_type: str) -> Dict[str, Any]:
        """Analyze the content of a file"""
        content_summary = {
            'file_type': file_type,
            'has_content': False,
            'content_length': 0,
            'key_metrics': {},
            'data_structure': {},
            'insights': []
        }
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                
            content_summary['has_content'] = True
            content_summary['content_length'] = len(content)
            
            # Analyze based on file type
            if file_type == 'json_data':
                content_summary.update(self.analyze_json_content(content))
            elif file_type == 'markdown_documentation':
                content_summary.update(self.analyze_markdown_content(content))
            elif file_type in ['python_code', 'javascript_code']:
                content_summary.update(self.analyze_code_content(content, file_type))
            
        except Exception as e:
            content_summary['error'] = str(e)
        
        return content_summary
    
    def analyze_json_content(self, content: str) -> Dict[str, Any]:
        """Analyze JSON content"""
        try:
            data = json.loads(content)
            
            analysis = {
                'data_type': 'json',
                'structure': self.analyze_json_structure(data),
                'key_fields': list(data.keys()) if isinstance(data, dict) else [],
                'data_size': len(str(data)),
                'nested_levels': self.count_nested_levels(data),
                'insights': []
            }
            
            # Extract insights from JSON data
            if isinstance(data, dict):
                if 'patterns' in data:
                    analysis['insights'].append(f"Contains {len(data['patterns'])} patterns")
                if 'correlations' in data:
                    analysis['insights'].append(f"Contains {len(data['correlations'])} correlations")
                if 'agent_summaries' in data:
                    analysis['insights'].append(f"Contains {len(data['agent_summaries'])} agent summaries")
                if 'scientific_breakthroughs' in data:
                    analysis['insights'].append(f"Contains {len(data['scientific_breakthroughs'])} scientific breakthroughs")
            
            return analysis
            
        except json.JSONDecodeError:
            return {'data_type': 'json', 'error': 'Invalid JSON'}
    
    def analyze_json_structure(self, data: Any, max_depth: int = 3) -> Dict[str, Any]:
        """Analyze JSON structure recursively"""
        if max_depth <= 0:
            return {'type': 'max_depth_reached'}
        
        if isinstance(data, dict):
            structure = {
                'type': 'object',
                'keys': list(data.keys()),
                'key_count': len(data),
                'nested': {}
            }
            
            for key, value in list(data.items())[:5]:  # Limit to first 5 keys
                structure['nested'][key] = self.analyze_json_structure(value, max_depth - 1)
            
            return structure
        elif isinstance(data, list):
            return {
                'type': 'array',
                'length': len(data),
                'sample_type': type(data[0]).__name__ if data else 'empty'
            }
        else:
            return {
                'type': type(data).__name__,
                'value_sample': str(data)[:100] if data else None
            }
    
    def count_nested_levels(self, data: Any, current_level: int = 0) -> int:
        """Count nested levels in JSON data"""
        if isinstance(data, dict):
            if not data:
                return current_level
            return max(self.count_nested_levels(value, current_level + 1) for value in data.values())
        elif isinstance(data, list):
            if not data:
                return current_level
            return max(self.count_nested_levels(item, current_level + 1) for item in data)
        else:
            return current_level
    
    def analyze_markdown_content(self, content: str) -> Dict[str, Any]:
        """Analyze markdown content"""
        lines = content.split('\n')
        
        analysis = {
            'data_type': 'markdown',
            'line_count': len(lines),
            'word_count': len(content.split()),
            'headers': [],
            'code_blocks': 0,
            'links': 0,
            'insights': []
        }
        
        # Count headers
        for line in lines:
            if line.startswith('#'):
                analysis['headers'].append(line.strip())
            if line.startswith('```'):
                analysis['code_blocks'] += 1
            if '[' in line and '](' in line:
                analysis['links'] += 1
        
        # Extract insights
        if 'consciousness' in content.lower():
            analysis['insights'].append('Contains consciousness-related content')
        if 'quantum' in content.lower():
            analysis['insights'].append('Contains quantum-related content')
        if 'breakthrough' in content.lower():
            analysis['insights'].append('Contains breakthrough-related content')
        if 'pattern' in content.lower():
            analysis['insights'].append('Contains pattern analysis content')
        
        return analysis
    
    def analyze_code_content(self, content: str, file_type: str) -> Dict[str, Any]:
        """Analyze code content"""
        lines = content.split('\n')
        
        analysis = {
            'data_type': file_type,
            'line_count': len(lines),
            'function_count': 0,
            'class_count': 0,
            'import_count': 0,
            'comment_lines': 0,
            'insights': []
        }
        
        # Count code elements
        for line in lines:
            stripped = line.strip()
            if stripped.startswith('def ') or stripped.startswith('function '):
                analysis['function_count'] += 1
            elif stripped.startswith('class '):
                analysis['class_count'] += 1
            elif stripped.startswith('import ') or stripped.startswith('from '):
                analysis['import_count'] += 1
            elif stripped.startswith('#') or stripped.startswith('//'):
                analysis['comment_lines'] += 1
        
        # Extract insights
        if 'consciousness' in content.lower():
            analysis['insights'].append('Contains consciousness-related code')
        if 'quantum' in content.lower():
            analysis['insights'].append('Contains quantum-related code')
        if 'breakthrough' in content.lower():
            analysis['insights'].append('Contains breakthrough-related code')
        if 'pattern' in content.lower():
            analysis['insights'].append('Contains pattern analysis code')
        
        return analysis
    
    def calculate_data_quality(self, file_path: str, content_summary: Dict[str, Any]) -> float:
        """Calculate data quality score"""
        score = 0.0
        
        # Base score for having content
        if content_summary.get('has_content', False):
            score += 0.3
        
        # Score for content length
        content_length = content_summary.get('content_length', 0)
        if content_length > 1000:
            score += 0.2
        elif content_length > 100:
            score += 0.1
        
        # Score for insights
        insights = content_summary.get('insights', [])
        score += min(0.3, len(insights) * 0.1)
        
        # Score for data structure complexity
        if 'structure' in content_summary:
            structure = content_summary['structure']
            if structure.get('key_count', 0) > 5:
                score += 0.1
            if structure.get('nested_levels', 0) > 2:
                score += 0.1
        
        return min(1.0, score)
    
    def calculate_system_metrics(self, data_sources: List[DataSource]) -> Dict[str, Any]:
        """Calculate overall system metrics"""
        total_files = len(data_sources)
        total_size = sum(ds.size_bytes for ds in data_sources)
        total_size_mb = total_size / (1024 * 1024)
        
        # Calculate quality scores
        quality_scores = [ds.data_quality_score for ds in data_sources]
        avg_quality = sum(quality_scores) / len(quality_scores) if quality_scores else 0
        
        # Count by file type
        file_types = defaultdict(int)
        for ds in data_sources:
            file_types[ds.file_type] += 1
        
        # Calculate system health
        system_health_score = self.calculate_system_health(data_sources)
        
        return {
            'total_files': total_files,
            'total_size_mb': total_size_mb,
            'average_quality_score': avg_quality,
            'file_type_distribution': dict(file_types),
            'system_health_score': system_health_score
        }
    
    def calculate_system_health(self, data_sources: List[DataSource]) -> float:
        """Calculate system health score"""
        health_score = 0.0
        
        # Check for essential file types
        file_types = [ds.file_type for ds in data_sources]
        
        if 'json_data' in file_types:
            health_score += 0.3
        if 'markdown_documentation' in file_types:
            health_score += 0.2
        if 'python_code' in file_types:
            health_score += 0.2
        if 'javascript_code' in file_types:
            health_score += 0.1
        
        # Check for recent activity
        current_time = time.time()
        recent_files = sum(1 for ds in data_sources if current_time - ds.last_modified < 86400)  # Last 24 hours
        if recent_files > 0:
            health_score += 0.2
        
        return min(1.0, health_score)
    
    def generate_cross_system_insights(self, data_sources: List[DataSource]) -> List[str]:
        """Generate cross-system insights"""
        insights = []
        
        # Analyze patterns across all data sources
        all_insights = []
        for ds in data_sources:
            all_insights.extend(ds.content_summary.get('insights', []))
        
        # Count insight frequencies
        insight_counts = defaultdict(int)
        for insight in all_insights:
            insight_counts[insight] += 1
        
        # Generate cross-system insights
        if insight_counts['Contains consciousness-related content'] > 3:
            insights.append("Strong consciousness mathematics integration across system")
        
        if insight_counts['Contains quantum-related content'] > 2:
            insights.append("Quantum mechanics deeply integrated in system architecture")
        
        if insight_counts['Contains breakthrough-related content'] > 2:
            insights.append("Multiple breakthrough patterns detected across system")
        
        if insight_counts['Contains pattern analysis content'] > 2:
            insights.append("Comprehensive pattern analysis capabilities demonstrated")
        
        # Check for data coherence
        json_files = [ds for ds in data_sources if ds.file_type == 'json_data']
        if len(json_files) > 5:
            insights.append("Rich structured data ecosystem with multiple JSON sources")
        
        # Check for documentation quality
        md_files = [ds for ds in data_sources if ds.file_type == 'markdown_documentation']
        if len(md_files) > 3:
            insights.append("Comprehensive documentation system in place")
        
        return insights
    
    def create_system_data(self, data_sources: List[DataSource], system_metrics: Dict[str, Any], insights: List[str]) -> SystemData:
        """Create comprehensive system data"""
        # Calculate data coherence score
        data_coherence_score = self.calculate_data_coherence(data_sources)
        
        # Generate breakthrough insights
        breakthrough_insights = self.extract_breakthrough_insights(data_sources)
        
        # Generate cross-system patterns
        cross_system_patterns = self.extract_cross_system_patterns(data_sources)
        
        # Generate quantum signatures
        quantum_signatures = self.generate_quantum_signatures(data_sources)
        
        return SystemData(
            total_files=system_metrics['total_files'],
            total_size_mb=system_metrics['total_size_mb'],
            data_sources=data_sources,
            system_health_score=system_metrics['system_health_score'],
            data_coherence_score=data_coherence_score,
            breakthrough_insights=breakthrough_insights,
            cross_system_patterns=cross_system_patterns,
            quantum_signatures=quantum_signatures
        )
    
    def calculate_data_coherence(self, data_sources: List[DataSource]) -> float:
        """Calculate data coherence score"""
        coherence_score = 0.0
        
        # Check for consistent file naming patterns
        file_names = [ds.name for ds in data_sources]
        
        # Check for timestamp patterns
        timestamp_files = [name for name in file_names if re.search(r'\d{10,}', name)]
        if len(timestamp_files) > 5:
            coherence_score += 0.3
        
        # Check for consistent file types
        file_types = [ds.file_type for ds in data_sources]
        type_counts = defaultdict(int)
        for file_type in file_types:
            type_counts[file_type] += 1
        
        if len(type_counts) >= 3:  # Good diversity
            coherence_score += 0.2
        
        # Check for quality consistency
        quality_scores = [ds.data_quality_score for ds in data_sources]
        if quality_scores:
            quality_variance = sum((score - sum(quality_scores)/len(quality_scores))**2 for score in quality_scores) / len(quality_scores)
            if quality_variance < 0.1:  # Low variance indicates coherence
                coherence_score += 0.5
        
        return min(1.0, coherence_score)
    
    def extract_breakthrough_insights(self, data_sources: List[DataSource]) -> List[str]:
        """Extract breakthrough insights from all data sources"""
        insights = []
        
        for ds in data_sources:
            content_summary = ds.content_summary
            
            # Check for breakthrough-related content
            if 'breakthrough' in ds.name.lower():
                insights.append(f"Breakthrough data found in {ds.name}")
            
            # Check content insights
            for insight in content_summary.get('insights', []):
                if 'breakthrough' in insight.lower():
                    insights.append(f"Breakthrough insight: {insight} (from {ds.name})")
            
            # Check for high-quality data
            if ds.data_quality_score > 0.8:
                insights.append(f"High-quality data source: {ds.name} (score: {ds.data_quality_score:.2f})")
        
        return list(set(insights))  # Remove duplicates
    
    def extract_cross_system_patterns(self, data_sources: List[DataSource]) -> List[Dict[str, Any]]:
        """Extract cross-system patterns"""
        patterns = []
        
        # Pattern 1: Consciousness integration
        consciousness_files = [ds for ds in data_sources if any('consciousness' in insight.lower() for insight in ds.content_summary.get('insights', []))]
        if len(consciousness_files) > 2:
            patterns.append({
                'pattern_type': 'consciousness_integration',
                'description': f"Consciousness mathematics integrated across {len(consciousness_files)} files",
                'strength': min(1.0, len(consciousness_files) / 5.0),
                'files': [ds.name for ds in consciousness_files]
            })
        
        # Pattern 2: Quantum integration
        quantum_files = [ds for ds in data_sources if any('quantum' in insight.lower() for insight in ds.content_summary.get('insights', []))]
        if len(quantum_files) > 1:
            patterns.append({
                'pattern_type': 'quantum_integration',
                'description': f"Quantum mechanics integrated across {len(quantum_files)} files",
                'strength': min(1.0, len(quantum_files) / 3.0),
                'files': [ds.name for ds in quantum_files]
            })
        
        # Pattern 3: Pattern analysis
        pattern_files = [ds for ds in data_sources if any('pattern' in insight.lower() for insight in ds.content_summary.get('insights', []))]
        if len(pattern_files) > 1:
            patterns.append({
                'pattern_type': 'pattern_analysis',
                'description': f"Pattern analysis capabilities across {len(pattern_files)} files",
                'strength': min(1.0, len(pattern_files) / 3.0),
                'files': [ds.name for ds in pattern_files]
            })
        
        return patterns
    
    def generate_quantum_signatures(self, data_sources: List[DataSource]) -> Dict[str, float]:
        """Generate quantum signatures for the system"""
        signatures = {}
        
        # Calculate quantum coherence from file quality scores
        quality_scores = [ds.data_quality_score for ds in data_sources]
        if quality_scores:
            signatures['quantum_coherence'] = sum(quality_scores) / len(quality_scores)
        
        # Calculate consciousness alignment
        consciousness_files = [ds for ds in data_sources if any('consciousness' in insight.lower() for insight in ds.content_summary.get('insights', []))]
        signatures['consciousness_alignment'] = len(consciousness_files) / len(data_sources) if data_sources else 0
        
        # Calculate breakthrough potential
        breakthrough_files = [ds for ds in data_sources if any('breakthrough' in insight.lower() for insight in ds.content_summary.get('insights', []))]
        signatures['breakthrough_potential'] = len(breakthrough_files) / len(data_sources) if data_sources else 0
        
        # Calculate system complexity
        total_size = sum(ds.size_bytes for ds in data_sources)
        signatures['system_complexity'] = min(1.0, total_size / (1024 * 1024 * 10))  # Normalize to 10MB
        
        return signatures
    
    def save_comprehensive_report(self, system_data: SystemData):
        """Save comprehensive data scan report"""
        timestamp = int(time.time())
        filename = f"comprehensive_data_scan_report_{timestamp}.json"
        
        # Convert to JSON-serializable format
        report = {
            'scan_timestamp': timestamp,
            'total_files': system_data.total_files,
            'total_size_mb': system_data.total_size_mb,
            'system_health_score': system_data.system_health_score,
            'data_coherence_score': system_data.data_coherence_score,
            'data_sources': [
                {
                    'name': ds.name,
                    'file_path': ds.file_path,
                    'file_type': ds.file_type,
                    'size_bytes': ds.size_bytes,
                    'last_modified': ds.last_modified,
                    'data_quality_score': ds.data_quality_score,
                    'content_summary': ds.content_summary
                }
                for ds in system_data.data_sources
            ],
            'breakthrough_insights': system_data.breakthrough_insights,
            'cross_system_patterns': system_data.cross_system_patterns,
            'quantum_signatures': system_data.quantum_signatures
        }
        
        with open(filename, 'w') as f:
            json.dump(report, f, indent=2)
        
        print(f"üíæ Comprehensive data scan report saved to: {filename}")
        return filename

def main():
    """Main comprehensive data scanning pipeline"""
    print("üîç COMPREHENSIVE DATA SCANNER")
    print("Divine Calculus Engine - Complete Data Analysis")
    print("=" * 70)
    
    # Initialize scanner
    scanner = ComprehensiveDataScanner()
    
    # Perform comprehensive scan
    system_data = scanner.scan_all_data()
    
    # Save comprehensive report
    report_file = scanner.save_comprehensive_report(system_data)
    
    # Print summary
    print("\nüåü COMPREHENSIVE DATA SCAN COMPLETE!")
    print(f"üìä Total files scanned: {system_data.total_files}")
    print(f"üíæ Total data size: {system_data.total_size_mb:.2f} MB")
    print(f"üè• System health score: {system_data.system_health_score:.3f}")
    print(f"üîó Data coherence score: {system_data.data_coherence_score:.3f}")
    
    # Print breakthrough insights
    if system_data.breakthrough_insights:
        print(f"\nüí° Breakthrough insights found: {len(system_data.breakthrough_insights)}")
        for insight in system_data.breakthrough_insights[:5]:
            print(f"  ‚Ä¢ {insight}")
    
    # Print cross-system patterns
    if system_data.cross_system_patterns:
        print(f"\nüîó Cross-system patterns detected: {len(system_data.cross_system_patterns)}")
        for pattern in system_data.cross_system_patterns:
            print(f"  ‚Ä¢ {pattern['pattern_type']}: {pattern['description']} (strength: {pattern['strength']:.3f})")
    
    # Print quantum signatures
    print(f"\nüåå Quantum signatures:")
    for signature, value in system_data.quantum_signatures.items():
        print(f"  ‚Ä¢ {signature}: {value:.3f}")
    
    print(f"\nüåü The Divine Calculus Engine has successfully scanned all data!")
    print(f"üìã Complete report saved to: {report_file}")

if __name__ == "__main__":
    main()
